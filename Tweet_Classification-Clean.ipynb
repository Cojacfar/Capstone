{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "\n",
    "The first step is to take the data and put it into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@azuresupport #azhelp:</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@AzureSupport, trying to activate a subscripti...</td>\n",
       "      <td>Subscription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@justinchronicle Hi Justin, our friends @Azure...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@azuresupport #AHD:PT41-JL8</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@AzureSupport just a heads up, but this button...</td>\n",
       "      <td>Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@AzureSupport I am getting Write DomainService...</td>\n",
       "      <td>AAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TweetText       Category\n",
       "50                          @azuresupport #azTechHelp  Other - MS PG\n",
       "51                             @azuresupport #azhelp:  Other - MS PG\n",
       "52  @AzureSupport, trying to activate a subscripti...   Subscription\n",
       "53  @justinchronicle Hi Justin, our friends @Azure...  Other - MS PG\n",
       "54                          @azuresupport #azTechHelp  Other - MS PG\n",
       "55                          @azuresupport #azTechHelp  Other - MS PG\n",
       "56                        @azuresupport #AHD:PT41-JL8  Other - MS PG\n",
       "57  ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...  Other - MS PG\n",
       "58  @AzureSupport just a heads up, but this button...        Support\n",
       "59  @AzureSupport I am getting Write DomainService...            AAD"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors #For word2vec static vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#CONSTANTS\n",
    "BASE_DIR = '../../Dataset/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "WEIGHTS_PATH = 'saved_models/weights.best.tweet_classification.hdf5'\n",
    "NUM_OF_RANKS = 25\n",
    "\n",
    "#Input Data, replace empty with NaN, and then drop these NaN fields so we don't train off non-categorized data.\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "df.dropna(axis=0, how=\"any\", subset=['Category'])\n",
    "data = df.drop(columns=['Date', 'Twitter Handle', 'Link', 'Reach', 'Customer Experience', 'Sentiment', 'Type', 'Total Outbound Tweets'])\n",
    "data[50:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "\n",
    "\n",
    "data['Category'] = pd.Categorical(data['Category'])\n",
    "data['coded'] = data['Category'].cat.codes\n",
    "label = np_utils.to_categorical(data['coded'].as_matrix())\n",
    "\n",
    "coded = dict(enumerate(data['Category'].cat.categories))\n",
    "\n",
    "#I need a list of categories to make TreeMap in d3.js\n",
    "unique_categories = [x for x in coded.values()]\n",
    "cat_tokens = {}\n",
    "cat_values = {}\n",
    "\n",
    "cats = le.fit_transform(data['coded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!\n",
      "[1, 57, 68, 320, 3, 385, 897, 9, 5, 36, 355, 60, 4, 41, 148, 5, 556, 556, 8, 1078, 339, 46, 4, 132, 6, 2185, 28, 1115, 12, 1966, 160, 52, 1404]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1 2377\n",
      "   42   22  179  230   86   50   34   13   32   75  191   30  201    9\n",
      "    5 1134  162  135    9  311  653  240]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat = df['TweetText'].tolist()\n",
    "data_1 = data['TweetText'].tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(flat)\n",
    "seq = tokenizer.texts_to_sequences(data_1)\n",
    "print(data_1[12])\n",
    "print(seq[12])\n",
    "\n",
    "#Initialize Word2Vec as a KeyedVector as I won't be using it as an object\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, label, test_size=0.2)\n",
    "print(X_train[12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 12101\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIMENSIONS))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_20 (InputLayer)            (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)         (None, 50, 300)       6340200     input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)             (None, 300, 50, 1)    0           embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)               (None, 300, 50, 512)  307712      reshape_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)               (None, 300, 50, 512)  614912      reshape_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)               (None, 300, 50, 512)  768512      reshape_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling2D)  (None, 30, 50, 512)   0           conv2d_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling2D)  (None, 30, 50, 512)   0           conv2d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling2D)  (None, 30, 50, 512)   0           conv2d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "average_12 (Average)             (None, 30, 50, 512)   0           max_pooling2d_50[0][0]           \n",
      "                                                                   max_pooling2d_51[0][0]           \n",
      "                                                                   max_pooling2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_9 (GlobalMa (None, 512)           0           average_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 256)           131328      global_max_pooling2d_9[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 98)            25186       dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 98)            0           dense_21[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 8,187,850\n",
      "Trainable params: 1,847,650\n",
      "Non-trainable params: 6,340,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Embedding, Reshape, Activation, MaxPooling2D, average\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import keras\n",
    "\n",
    "#Define Metric for matching accuracy of top 3\n",
    "inTop3 = lambda x, y: top_k_categorical_accuracy(x, y, k=3)\n",
    "\n",
    "input_tweet = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIMENSIONS,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_tweet)\n",
    "dat = Reshape((EMBEDDING_DIMENSIONS,MAX_SEQUENCE_LENGTH,1))(embedding_layer)\n",
    "\n",
    "tower_1 = Conv2D(512, (300, 2), strides=(1,1), padding='same', activation='relu')(dat)\n",
    "pool_1 = MaxPooling2D(pool_size=(10,1))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(512, (300, 4), strides=(1,1), padding='same', activation='relu')(dat)\n",
    "pool_2 = MaxPooling2D(pool_size=(10,1))(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(512, (300, 5), strides=(1,1), padding='same', activation='relu')(dat)\n",
    "pool_3 = MaxPooling2D(pool_size=(10,1))(tower_3)\n",
    "\n",
    "#cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "cat = average([pool_1, pool_2, pool_3])\n",
    "pool = GlobalMaxPooling2D()(cat)\n",
    "d_1 = Dense(256)(pool)\n",
    "dense = Dense(98)(d_1)\n",
    "out = Activation('softmax')(dense)\n",
    "model_functional = Model(inputs=input_tweet, outputs=out)\n",
    "model_functional.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy', inTop3])\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12324 samples, validate on 3081 samples\n",
      "Epoch 1/1\n",
      "10500/12324 [========================>.....] - ETA: 48s - loss: 3.2524 - acc: 0.2335 - categorical_accuracy: 0.2335 - <lambda>: 0.4223"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tweet_classification_clean.hdf5', \n",
    "                    verbose=1, save_best_only=True)\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "#X_train = np.expand_dims(X_train, axis=2)\n",
    "model_functional.fit(X_train, y_train, validation_split=.2, batch_size=25, callbacks=[checkpointer, stopper], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 3 Categories are ['Web Apps', 'VM', 'Subscription']\n"
     ]
    }
   ],
   "source": [
    "model_functional.save('saved_models/classification_model.h5')\n",
    "\n",
    "def make_cat(pred):\n",
    "    #Takes output prediction Matrix, finds 3 highest values and turns them into the matching category from the previous one-hot encoding\n",
    "    ind = np.argpartition(pred[0], -3)[-3:]\n",
    "    #arr = pred[0][ind]\n",
    "    labels = [coded[int(x)] for x in ind[::-1]]\n",
    "    return labels\n",
    "\n",
    "tweet = ['@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!']\n",
    "pred = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categ = model_functional.predict(np.array(pred))\n",
    "print(\"The Top 3 Categories are {0}\".format(make_cat(categ)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#trained_model = load_model('saved_models/classification_model.h5')\n",
    "#trained_model.summary()\n",
    "#trained_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
