{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "\n",
    "The first step is to take the data and put it into a pandas dataframe. We will then remove irrelevant columsn from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@azuresupport #azhelp:</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@AzureSupport, trying to activate a subscripti...</td>\n",
       "      <td>Subscription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@justinchronicle Hi Justin, our friends @Azure...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@azuresupport #AHD:PT41-JL8</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@AzureSupport just a heads up, but this button...</td>\n",
       "      <td>Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@AzureSupport I am getting Write DomainService...</td>\n",
       "      <td>AAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TweetText       Category\n",
       "50                          @azuresupport #azTechHelp  Other - MS PG\n",
       "51                             @azuresupport #azhelp:  Other - MS PG\n",
       "52  @AzureSupport, trying to activate a subscripti...   Subscription\n",
       "53  @justinchronicle Hi Justin, our friends @Azure...  Other - MS PG\n",
       "54                          @azuresupport #azTechHelp  Other - MS PG\n",
       "55                          @azuresupport #azTechHelp  Other - MS PG\n",
       "56                        @azuresupport #AHD:PT41-JL8  Other - MS PG\n",
       "57  ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...  Other - MS PG\n",
       "58  @AzureSupport just a heads up, but this button...        Support\n",
       "59  @AzureSupport I am getting Write DomainService...            AAD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors #For word2vec static vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#CONSTANTS\n",
    "BASE_DIR = '../../Dataset/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "WEIGHTS_PATH = 'saved_models/weights.best.tweet_classification.hdf5'\n",
    "NUM_OF_RANKS = 25\n",
    "MIN_FREQ = 50 #Drop any category with less than 50 samples\n",
    "\n",
    "#Input Data, replace empty with NaN, and then drop these NaN fields so we don't train off non-categorized data.\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "df.dropna(axis=0, how=\"any\", subset=['Category'])\n",
    "df = df.drop(columns=['Date', 'Twitter Handle', 'Link', 'Reach', 'Customer Experience', 'Sentiment', 'Type', 'Total Outbound Tweets'])\n",
    "df[50:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories to Remove: []\n",
      "Number of Categories: 42\n",
      "                                            TweetText  \\\n",
      "20  @azuresupport #azTechHelp we have no connectio...   \n",
      "21  @AzureSupport Hi! Migrated a Kali Linux VM fro...   \n",
      "22  @azuresupport #azhelp:\\nWhats happening with c...   \n",
      "23  @AzureSupport @andrewwatt BTW, its been logged...   \n",
      "24  @azuresupport #azTechHelp The portal is having...   \n",
      "25  @AzureSupport trying to upload my local window...   \n",
      "26                          @azuresupport #azTechHelp   \n",
      "27  @azuresupport #azTechHelp Estou utilizando um ...   \n",
      "28  @TheRegister are you aware of an Azure VM outa...   \n",
      "29  .@Azure Maybe raise awareness on this? All App...   \n",
      "\n",
      "                       Category  coded  \n",
      "20                           VM     36  \n",
      "21                           VM     36  \n",
      "22                           VM     36  \n",
      "23                      Support     35  \n",
      "24                       Backup      7  \n",
      "25                           VM     36  \n",
      "26                Other - MS PG     27  \n",
      "27  Visual Studio Team Services     40  \n",
      "28                           VM     36  \n",
      "29                  App Service      2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "\n",
    "values = df['Category'].value_counts()\n",
    "tuples = [tuple((x, y)) for x, y in values.items()]\n",
    "\n",
    "cat_to_remove = []\n",
    "#Making a list of each category that has less than my defined minimum samples\n",
    "for a,b in tuples:\n",
    "    if b < MIN_FREQ:\n",
    "        cat_to_remove.append(a)\n",
    "    \n",
    "print(\"Categories to Remove: {}\".format(cat_to_remove))\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "\n",
    "df['Category'] = pd.Categorical(df['Category'])\n",
    "df['coded'] = df['Category'].cat.codes\n",
    "#label = np_utils.to_categorical(data['coded'].as_matrix())\n",
    "\n",
    "\n",
    "coded = dict(enumerate(df['Category'].cat.categories))\n",
    "\n",
    "#I need a list of categories to make TreeMap in d3.js\n",
    "unique_categories = [x for x in coded.values()]\n",
    "num_of_cat = len(unique_categories)\n",
    "print(\"Number of Categories: {}\".format(num_of_cat))\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "print(df[20:30])\n",
    "\n",
    "label = np_utils.to_categorical(df['coded'].as_matrix())\n",
    "\n",
    "cats = le.fit_transform(df['coded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!\n",
      "[1, 59, 69, 323, 3, 462, 909, 9, 5, 36, 342, 61, 4, 42, 153, 5, 600, 600, 8, 1073, 345, 48, 4, 131, 6, 2101, 28, 1074, 12, 1890, 160, 52, 1326]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat = df['TweetText'].tolist()\n",
    "data_1 = df['TweetText'].tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(flat)\n",
    "seq = tokenizer.texts_to_sequences(data_1)\n",
    "print(data_1[12])\n",
    "print(seq[12])\n",
    "\n",
    "#Initialize Word2Vec as a KeyedVector as I won't be using it as an object\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, label, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 11441\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIMENSIONS))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 50, 300)       6071100     input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 300, 50, 1)    0           embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, 300, 50, 512)  307712      reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)               (None, 300, 50, 512)  614912      reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)               (None, 300, 50, 512)  768512      reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D)  (None, 30, 25, 512)   0           conv2d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D)  (None, 30, 25, 512)   0           conv2d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D)  (None, 30, 25, 512)   0           conv2d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 90, 25, 512)   0           max_pooling2d_25[0][0]           \n",
      "                                                                   max_pooling2d_26[0][0]           \n",
      "                                                                   max_pooling2d_27[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_9 (GlobalMa (None, 512)           0           concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 42)            21546       global_max_pooling2d_9[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 42)            0           dense_14[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 7,783,782\n",
      "Trainable params: 1,712,682\n",
      "Non-trainable params: 6,071,100\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Embedding, Reshape, Activation, MaxPooling2D, average\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import keras\n",
    "\n",
    "#Define Metric for matching accuracy of top 3\n",
    "inTop3 = lambda x, y: top_k_categorical_accuracy(x, y, k=3)\n",
    "\n",
    "input_tweet = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIMENSIONS,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_tweet)\n",
    "dat = Reshape((EMBEDDING_DIMENSIONS,MAX_SEQUENCE_LENGTH,1))(embedding_layer)\n",
    "\n",
    "tower_1 = Conv2D(512, (300, 2), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_1 = MaxPooling2D(pool_size=(10,2))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(512, (300, 4), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_2 = MaxPooling2D(pool_size=(10,2))(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(512, (300, 5), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_3 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "#cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "pool = GlobalMaxPooling2D()(cat)\n",
    "d_1 = Dense(256)(pool)\n",
    "dense = Dense(num_of_cat)(pool)\n",
    "out = Activation('softmax')(dense)\n",
    "model_functional = Model(inputs=input_tweet, outputs=out)\n",
    "model_functional.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy', inTop3])\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11592 samples, validate on 2898 samples\n",
      "Epoch 1/1\n",
      "11575/11592 [============================>.] - ETA: 0s - loss: 2.9114 - acc: 0.2544 - categorical_accuracy: 0.2544 - <lambda>: 0.4797Epoch 00000: val_loss improved from inf to 2.76750, saving model to saved_models/weights.best.tweet_classification_clean.hdf5\n",
      "11592/11592 [==============================] - 353s - loss: 2.9107 - acc: 0.2546 - categorical_accuracy: 0.2546 - <lambda>: 0.4802 - val_loss: 2.7675 - val_acc: 0.2954 - val_categorical_accuracy: 0.2954 - val_<lambda>: 0.5072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1640b66f208>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tweet_classification_clean.hdf5', \n",
    "                    verbose=1, save_best_only=True)\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "#X_train = np.expand_dims(X_train, axis=2)\n",
    "model_functional.fit(X_train, y_train, validation_split=.2, batch_size=25, callbacks=[checkpointer, stopper], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_functional.save('saved_models/classification_model.h5')\n",
    "\n",
    "def make_cat(pred):\n",
    "    #Takes output prediction Matrix, finds 3 highest values and turns them into the matching category from the previous one-hot encoding\n",
    "    ind = np.argpartition(pred[0], -3)[-3:]\n",
    "    #arr = pred[0][ind]\n",
    "    labels = [coded[int(x)] for x in ind[::-1]]\n",
    "    return labels\n",
    "\n",
    "tweet = ['@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!']\n",
    "pred = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categ = model_functional.predict(np.array(pred))\n",
    "print(\"The Top 3 Categories are {0}\".format(make_cat(categ)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#trained_model = load_model('saved_models/classification_model.h5')\n",
    "#trained_model.summary()\n",
    "#trained_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
