{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "\n",
    "The first step is to take the data and put it into a pandas dataframe. We will then remove irrelevant columsn from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@azuresupport #azhelp:</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@AzureSupport, trying to activate a subscripti...</td>\n",
       "      <td>Subscription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@justinchronicle Hi Justin, our friends @Azure...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@azuresupport #AHD:PT41-JL8</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@AzureSupport just a heads up, but this button...</td>\n",
       "      <td>Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@AzureSupport I am getting Write DomainService...</td>\n",
       "      <td>AAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TweetText       Category\n",
       "50                          @azuresupport #azTechHelp  Other - MS PG\n",
       "51                             @azuresupport #azhelp:  Other - MS PG\n",
       "52  @AzureSupport, trying to activate a subscripti...   Subscription\n",
       "53  @justinchronicle Hi Justin, our friends @Azure...  Other - MS PG\n",
       "54                          @azuresupport #azTechHelp  Other - MS PG\n",
       "55                          @azuresupport #azTechHelp  Other - MS PG\n",
       "56                        @azuresupport #AHD:PT41-JL8  Other - MS PG\n",
       "57  ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...  Other - MS PG\n",
       "58  @AzureSupport just a heads up, but this button...        Support\n",
       "59  @AzureSupport I am getting Write DomainService...            AAD"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors #For word2vec static vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#CONSTANTS\n",
    "BASE_DIR = '../../Dataset/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "WEIGHTS_PATH = 'saved_models/weights.best.tweet_classification.hdf5'\n",
    "NUM_OF_RANKS = 25\n",
    "MIN_FREQ = 100 #Drop any category with less than 50 samples\n",
    "\n",
    "#Input Data, replace empty with NaN, and then drop these NaN fields so we don't train off non-categorized data.\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "df.dropna(axis=0, how=\"any\", subset=['Category'])\n",
    "df = df.drop(columns=['Date', 'Twitter Handle', 'Link', 'Reach', 'Customer Experience', 'Sentiment', 'Type', 'Total Outbound Tweets'])\n",
    "df[50:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories to Remove: ['Bot Framework', 'CDN', 'Azure Resources Manager', 'IOT Hub', 'HDInsight', 'VPN Gateway', 'SR Complaint', 'Container Service', 'MySQL', 'Site Recovery', 'Notification Hubs', 'Logic Apps', 'Marketplace', 'Media Services', 'Machine Learning', 'Visual Studio Application', 'Data Factory', 'Redis Cache', 'Service Fabric', 'Power BI Embedded', 'PowerShell', 'DreamSpark', 'Free Trial', 'Outage', 'Application Gateway', 'Automation', 'CosmosDB', 'Clear DB', 'Marketing', 'Azure Stack', 'Key Vault', 'Mobile Apps', 'Powershell', 'Azure Security Center', 'Mobile Services', 'Log Analytics', 'Multi-Factor Authentication', 'Imagine', 'Azure Search', 'Remote App', 'Azure DB', 'Load Balancer', 'WordPress', 'Stream Analytics', 'Data Lake', 'Traffic Manager', 'Event Hubs', 'Sign Up Issue', 'Ibizia', 'ExpressRoute', 'Azure Machine Learning', 'Azure Analysis', 'SQL Data Warehouse', 'Monitoring', 'Compliance', 'Azure DevTest Labs', 'Scheduler', 'MSDN', 'Operational Insights', 'Batch', 'Access Control Service', 'AutoScale', 'ZUMO App', 'StorSimple', 'SQL Stretch Server', 'Data Catalog', 'Mobile Engagement', 'HockeyApp', 'Virtual Machine Scale Sets', 'Biztalk Services', 'Minecraft', 'Alerts', 'Manage Cache']\n",
      "Number of Categories: 25\n",
      "                                            TweetText  \\\n",
      "20  @azuresupport #azTechHelp we have no connectio...   \n",
      "21  @AzureSupport Hi! Migrated a Kali Linux VM fro...   \n",
      "22  @azuresupport #azhelp:\\nWhats happening with c...   \n",
      "23  @AzureSupport @andrewwatt BTW, its been logged...   \n",
      "24  @azuresupport #azTechHelp The portal is having...   \n",
      "25  @AzureSupport trying to upload my local window...   \n",
      "26                          @azuresupport #azTechHelp   \n",
      "27  @azuresupport #azTechHelp Estou utilizando um ...   \n",
      "28  @TheRegister are you aware of an Azure VM outa...   \n",
      "29  .@Azure Maybe raise awareness on this? All App...   \n",
      "\n",
      "                       Category  coded  \n",
      "20                           VM     21  \n",
      "21                           VM     21  \n",
      "22                           VM     21  \n",
      "23                      Support     20  \n",
      "24                       Backup      6  \n",
      "25                           VM     21  \n",
      "26                Other - MS PG     14  \n",
      "27  Visual Studio Team Services     23  \n",
      "28                           VM     21  \n",
      "29                  App Service      2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "\n",
    "values = df['Category'].value_counts()\n",
    "tuples = [tuple((x, y)) for x, y in values.items()]\n",
    "\n",
    "cat_to_remove = []\n",
    "#Making a list of each category that has less than my defined minimum samples\n",
    "for a,b in tuples:\n",
    "    if b < MIN_FREQ:\n",
    "        cat_to_remove.append(a)\n",
    "    \n",
    "print(\"Categories to Remove: {}\".format(cat_to_remove))\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "\n",
    "df['Category'] = pd.Categorical(df['Category'])\n",
    "df['coded'] = df['Category'].cat.codes\n",
    "#label = np_utils.to_categorical(data['coded'].as_matrix())\n",
    "\n",
    "\n",
    "coded = dict(enumerate(df['Category'].cat.categories))\n",
    "\n",
    "#I need a list of categories to make TreeMap in d3.js\n",
    "unique_categories = [x for x in coded.values()]\n",
    "num_of_cat = len(unique_categories)\n",
    "print(\"Number of Categories: {}\".format(num_of_cat))\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "print(df[20:30])\n",
    "\n",
    "label = np_utils.to_categorical(df['coded'].as_matrix())\n",
    "\n",
    "cats = le.fit_transform(df['coded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!\n",
      "[1, 59, 70, 317, 3, 439, 844, 9, 5, 36, 354, 60, 4, 42, 152, 5, 650, 650, 8, 1013, 339, 50, 4, 127, 6, 2006, 29, 1159, 12, 1900, 166, 54, 1450]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat = df['TweetText'].tolist()\n",
    "data_1 = df['TweetText'].tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(flat)\n",
    "seq = tokenizer.texts_to_sequences(data_1)\n",
    "print(data_1[12])\n",
    "print(seq[12])\n",
    "\n",
    "#Initialize Word2Vec as a KeyedVector as I won't be using it as an object\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, label, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 10752\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIMENSIONS))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 50, 300)       5781900     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 300, 50, 1)    0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 300, 50, 512)  307712      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 300, 50, 512)  614912      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 300, 50, 512)  768512      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 100, 512)  0           max_pooling2d_1[0][0]            \n",
      "                                                                   max_pooling2d_2[0][0]            \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "                                                                   max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 30, 100, 512)  262656      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalMa (None, 512)           0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 25)            12825       global_max_pooling2d_1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 25)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 7,748,517\n",
      "Trainable params: 1,966,617\n",
      "Non-trainable params: 5,781,900\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Embedding, Reshape, Activation, MaxPooling2D, average, Dropout\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import keras\n",
    "\n",
    "#Define Metric for matching accuracy of top 3\n",
    "inTop3 = lambda x, y: top_k_categorical_accuracy(x, y, k=3)\n",
    "\n",
    "input_tweet = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIMENSIONS,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_tweet)\n",
    "dat = Reshape((EMBEDDING_DIMENSIONS,MAX_SEQUENCE_LENGTH,1))(embedding_layer)\n",
    "\n",
    "tower_1 = Conv2D(512, (300, 2), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_1 = MaxPooling2D(pool_size=(10,2))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(512, (300, 4), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_2 = MaxPooling2D(pool_size=(10,2))(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(512, (300, 5), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_3 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "tower_4 = Conv2D(512, (300,1), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_4 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "#cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "cat = keras.layers.concatenate([pool_1, pool_2, pool_3, pool_4], axis=2)\n",
    "conv = Conv2D(512, (1,1), strides=1, padding='same', activation='relu')(cat)\n",
    "pool = GlobalMaxPooling2D()(conv)\n",
    "d_1 = Dense(256)(pool)\n",
    "drop = Dropout(.2)(d_1)\n",
    "dense = Dense(num_of_cat)(drop)\n",
    "out = Activation('softmax')(dense)\n",
    "model_functional = Model(inputs=input_tweet, outputs=out)\n",
    "model_functional.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy', inTop3])\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10825 samples, validate on 2707 samples\n",
      "Epoch 1/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 2.5997 - acc: 0.2767 - categorical_accuracy: 0.2767 - <lambda>: 0.5055Epoch 00000: val_loss improved from inf to 2.38977, saving model to saved_models/weights.best.tweet_classification_clean.hdf5\n",
      "10825/10825 [==============================] - 353s - loss: 2.5998 - acc: 0.2767 - categorical_accuracy: 0.2767 - <lambda>: 0.5056 - val_loss: 2.3898 - val_acc: 0.3557 - val_categorical_accuracy: 0.3557 - val_<lambda>: 0.5427\n",
      "Epoch 2/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 2.2127 - acc: 0.3783 - categorical_accuracy: 0.3783 - <lambda>: 0.5990Epoch 00001: val_loss improved from 2.38977 to 2.20530, saving model to saved_models/weights.best.tweet_classification_clean.hdf5\n",
      "10825/10825 [==============================] - 352s - loss: 2.2129 - acc: 0.3782 - categorical_accuracy: 0.3782 - <lambda>: 0.5990 - val_loss: 2.2053 - val_acc: 0.3919 - val_categorical_accuracy: 0.3919 - val_<lambda>: 0.6007\n",
      "Epoch 3/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 1.8896 - acc: 0.4626 - categorical_accuracy: 0.4626 - <lambda>: 0.6815Epoch 00002: val_loss did not improve\n",
      "10825/10825 [==============================] - 352s - loss: 1.8898 - acc: 0.4625 - categorical_accuracy: 0.4625 - <lambda>: 0.6815 - val_loss: 2.2068 - val_acc: 0.4100 - val_categorical_accuracy: 0.4100 - val_<lambda>: 0.6165\n",
      "Epoch 4/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 1.4848 - acc: 0.5792 - categorical_accuracy: 0.5792 - <lambda>: 0.7804Epoch 00003: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 1.4850 - acc: 0.5792 - categorical_accuracy: 0.5792 - <lambda>: 0.7804 - val_loss: 2.9313 - val_acc: 0.2756 - val_categorical_accuracy: 0.2756 - val_<lambda>: 0.5840\n",
      "Epoch 5/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 1.0487 - acc: 0.7123 - categorical_accuracy: 0.7123 - <lambda>: 0.8676Epoch 00004: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 1.0484 - acc: 0.7123 - categorical_accuracy: 0.7123 - <lambda>: 0.8676 - val_loss: 2.3566 - val_acc: 0.4004 - val_categorical_accuracy: 0.4004 - val_<lambda>: 0.6202\n",
      "Epoch 6/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.6835 - acc: 0.8222 - categorical_accuracy: 0.8222 - <lambda>: 0.9248Epoch 00005: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.6833 - acc: 0.8222 - categorical_accuracy: 0.8222 - <lambda>: 0.9248 - val_loss: 3.6615 - val_acc: 0.3609 - val_categorical_accuracy: 0.3609 - val_<lambda>: 0.6021\n",
      "Epoch 7/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.4575 - acc: 0.8805 - categorical_accuracy: 0.8805 - <lambda>: 0.9520Epoch 00006: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.4573 - acc: 0.8806 - categorical_accuracy: 0.8806 - <lambda>: 0.9521 - val_loss: 3.1994 - val_acc: 0.3731 - val_categorical_accuracy: 0.3731 - val_<lambda>: 0.5704\n",
      "Epoch 8/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.9031 - categorical_accuracy: 0.9031 - <lambda>: 0.9616Epoch 00007: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.3592 - acc: 0.9032 - categorical_accuracy: 0.9032 - <lambda>: 0.9616 - val_loss: 3.5547 - val_acc: 0.3613 - val_categorical_accuracy: 0.3613 - val_<lambda>: 0.5763\n",
      "Epoch 9/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9143 - categorical_accuracy: 0.9143 - <lambda>: 0.9628Epoch 00008: val_loss did not improve\n",
      "10825/10825 [==============================] - 355s - loss: 0.3093 - acc: 0.9144 - categorical_accuracy: 0.9144 - <lambda>: 0.9628 - val_loss: 3.3107 - val_acc: 0.3849 - val_categorical_accuracy: 0.3849 - val_<lambda>: 0.5608\n",
      "Epoch 10/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9178 - categorical_accuracy: 0.9178 - <lambda>: 0.9644Epoch 00009: val_loss did not improve\n",
      "10825/10825 [==============================] - 355s - loss: 0.2916 - acc: 0.9178 - categorical_accuracy: 0.9178 - <lambda>: 0.9644 - val_loss: 3.6678 - val_acc: 0.3967 - val_categorical_accuracy: 0.3967 - val_<lambda>: 0.6095\n",
      "Epoch 11/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9209 - categorical_accuracy: 0.9209 - <lambda>: 0.9648Epoch 00010: val_loss did not improve\n",
      "10825/10825 [==============================] - 354s - loss: 0.2822 - acc: 0.9208 - categorical_accuracy: 0.9208 - <lambda>: 0.9647 - val_loss: 3.7213 - val_acc: 0.3668 - val_categorical_accuracy: 0.3668 - val_<lambda>: 0.5441\n",
      "Epoch 12/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9212 - categorical_accuracy: 0.9212 - <lambda>: 0.9653Epoch 00011: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.2857 - acc: 0.9212 - categorical_accuracy: 0.9212 - <lambda>: 0.9654 - val_loss: 3.6934 - val_acc: 0.3960 - val_categorical_accuracy: 0.3960 - val_<lambda>: 0.5877\n",
      "Epoch 13/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9219 - categorical_accuracy: 0.9219 - <lambda>: 0.9652Epoch 00012: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.2763 - acc: 0.9219 - categorical_accuracy: 0.9219 - <lambda>: 0.9652 - val_loss: 3.8441 - val_acc: 0.3967 - val_categorical_accuracy: 0.3967 - val_<lambda>: 0.5929\n",
      "Epoch 14/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9226 - categorical_accuracy: 0.9226 - <lambda>: 0.9659Epoch 00013: val_loss did not improve\n",
      "10825/10825 [==============================] - 354s - loss: 0.2748 - acc: 0.9226 - categorical_accuracy: 0.9226 - <lambda>: 0.9658 - val_loss: 4.3446 - val_acc: 0.3738 - val_categorical_accuracy: 0.3738 - val_<lambda>: 0.5752\n",
      "Epoch 15/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9213 - categorical_accuracy: 0.9213 - <lambda>: 0.9659Epoch 00014: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.2843 - acc: 0.9212 - categorical_accuracy: 0.9212 - <lambda>: 0.9659 - val_loss: 4.7412 - val_acc: 0.3960 - val_categorical_accuracy: 0.3960 - val_<lambda>: 0.6228\n",
      "Epoch 16/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9229 - categorical_accuracy: 0.9229 - <lambda>: 0.9666Epoch 00015: val_loss did not improve\n",
      "10825/10825 [==============================] - 352s - loss: 0.2722 - acc: 0.9230 - categorical_accuracy: 0.9230 - <lambda>: 0.9667 - val_loss: 4.3113 - val_acc: 0.3742 - val_categorical_accuracy: 0.3742 - val_<lambda>: 0.5600\n",
      "Epoch 17/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9209 - categorical_accuracy: 0.9209 - <lambda>: 0.9657Epoch 00016: val_loss did not improve\n",
      "10825/10825 [==============================] - 352s - loss: 0.2718 - acc: 0.9209 - categorical_accuracy: 0.9209 - <lambda>: 0.9657 - val_loss: 4.1554 - val_acc: 0.3816 - val_categorical_accuracy: 0.3816 - val_<lambda>: 0.5733\n",
      "Epoch 18/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9201 - categorical_accuracy: 0.9201 - <lambda>: 0.9662Epoch 00017: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.2798 - acc: 0.9201 - categorical_accuracy: 0.9201 - <lambda>: 0.9662 - val_loss: 4.2601 - val_acc: 0.3860 - val_categorical_accuracy: 0.3860 - val_<lambda>: 0.5652\n",
      "Epoch 19/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9210 - categorical_accuracy: 0.9210 - <lambda>: 0.9662Epoch 00018: val_loss did not improve\n",
      "10825/10825 [==============================] - 353s - loss: 0.2748 - acc: 0.9210 - categorical_accuracy: 0.9210 - <lambda>: 0.9662 - val_loss: 4.7044 - val_acc: 0.3908 - val_categorical_accuracy: 0.3908 - val_<lambda>: 0.5789\n",
      "Epoch 20/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9208 - categorical_accuracy: 0.9208 - <lambda>: 0.9659Epoch 00019: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10825/10825 [==============================] - 353s - loss: 0.2870 - acc: 0.9208 - categorical_accuracy: 0.9208 - <lambda>: 0.9659 - val_loss: 4.3912 - val_acc: 0.3931 - val_categorical_accuracy: 0.3931 - val_<lambda>: 0.5911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x216671f2cc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tweet_classification_clean.hdf5', \n",
    "                    verbose=1, save_best_only=True)\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "#X_train = np.expand_dims(X_train, axis=2)\n",
    "model_functional.fit(X_train, y_train, validation_split=.2, batch_size=20, epochs=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top 3 Categories are ['API Management', 'Web Apps', 'DNS']\n"
     ]
    }
   ],
   "source": [
    "##### model_functional.save('saved_models/classification_model.h5')\n",
    "\n",
    "def make_cat(pred):\n",
    "    #Takes output prediction Matrix, finds 3 highest values and turns them into the matching category from the previous one-hot encoding\n",
    "    ind = np.argpartition(pred[0], -3)[-3:]\n",
    "    #arr = pred[0][ind]\n",
    "    labels = [coded[int(x)] for x in ind[::-1]]\n",
    "    return labels\n",
    "\n",
    "tweet = ['@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!']\n",
    "pred = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categ = model_functional.predict(np.array(pred))\n",
    "print(\"The Top 3 Categories are {0}\".format(make_cat(categ)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#trained_model = load_model('saved_models/classification_model.h5')\n",
    "#trained_model.summary()\n",
    "#trained_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
