{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "The first step is to take the data and put it into a pandas dataframe. Once the data is into Jupyter, we can begin the process of cleaning and transforming the data into the format our CNN will require. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@azuresupport #azhelp:</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@AzureSupport, trying to activate a subscripti...</td>\n",
       "      <td>Subscription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@justinchronicle Hi Justin, our friends @Azure...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@azuresupport #AHD:PT41-JL8</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@AzureSupport just a heads up, but this button...</td>\n",
       "      <td>Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@AzureSupport I am getting Write DomainService...</td>\n",
       "      <td>AAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TweetText       Category\n",
       "50                          @azuresupport #azTechHelp  Other - MS PG\n",
       "51                             @azuresupport #azhelp:  Other - MS PG\n",
       "52  @AzureSupport, trying to activate a subscripti...   Subscription\n",
       "53  @justinchronicle Hi Justin, our friends @Azure...  Other - MS PG\n",
       "54                          @azuresupport #azTechHelp  Other - MS PG\n",
       "55                          @azuresupport #azTechHelp  Other - MS PG\n",
       "56                        @azuresupport #AHD:PT41-JL8  Other - MS PG\n",
       "57  ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...  Other - MS PG\n",
       "58  @AzureSupport just a heads up, but this button...        Support\n",
       "59  @AzureSupport I am getting Write DomainService...            AAD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors #For word2vec static vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#CONSTANTS\n",
    "BASE_DIR = '../../../Dataset/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "WEIGHTS_PATH = 'saved_models/weights.best.tweet_classification.hdf5'\n",
    "NUM_OF_RANKS = 25\n",
    "MIN_SAMPLES = 100 #Drop any category with less than 50 samples\n",
    "\n",
    "#Input Data, replace empty with NaN, and then drop these NaN fields so we don't train off non-categorized data.\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "df.dropna(axis=0, how=\"any\", subset=['Category'])\n",
    "df = df.drop(columns=['Date', 'Twitter Handle', 'Link', 'Reach', 'Customer Experience', 'Sentiment', 'Type', 'Total Outbound Tweets'])\n",
    "df[50:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories to Remove: ['Bot Framework', 'CDN', 'Azure Resources Manager', 'IOT Hub', 'VPN Gateway', 'HDInsight', 'SR Complaint', 'Container Service', 'MySQL', 'Site Recovery', 'Notification Hubs', 'Logic Apps', 'Marketplace', 'Media Services', 'Machine Learning', 'Visual Studio Application', 'Data Factory', 'Redis Cache', 'Service Fabric', 'Power BI Embedded', 'PowerShell', 'DreamSpark', 'Free Trial', 'Outage', 'Application Gateway', 'Automation', 'CosmosDB', 'Clear DB', 'Azure Stack', 'Marketing', 'Key Vault', 'Mobile Apps', 'Mobile Services', 'Azure Security Center', 'Powershell', 'Log Analytics', 'Multi-Factor Authentication', 'Imagine', 'Remote App', 'Azure Search', 'Azure DB', 'Load Balancer', 'WordPress', 'Data Lake', 'Stream Analytics', 'Traffic Manager', 'Event Hubs', 'Sign Up Issue', 'Ibizia', 'ExpressRoute', 'Azure Machine Learning', 'Azure Analysis', 'Monitoring', 'SQL Data Warehouse', 'Azure DevTest Labs', 'Compliance', 'Scheduler', 'Operational Insights', 'MSDN', 'Batch', 'Access Control Service', 'AutoScale', 'SQL Stretch Server', 'Data Catalog', 'StorSimple', 'ZUMO App', 'HockeyApp', 'Virtual Machine Scale Sets', 'Biztalk Services', 'Mobile Engagement', 'Minecraft', 'Manage Cache', 'Alerts']\n",
      "Number of Categories: 25\n",
      "                                            TweetText  \\\n",
      "20  @azuresupport #azTechHelp we have no connectio...   \n",
      "21  @AzureSupport Hi! Migrated a Kali Linux VM fro...   \n",
      "22  @azuresupport #azhelp:\\nWhats happening with c...   \n",
      "23  @AzureSupport @andrewwatt BTW, its been logged...   \n",
      "24  @azuresupport #azTechHelp The portal is having...   \n",
      "25  @AzureSupport trying to upload my local window...   \n",
      "26                          @azuresupport #azTechHelp   \n",
      "27  @azuresupport #azTechHelp Estou utilizando um ...   \n",
      "28  @TheRegister are you aware of an Azure VM outa...   \n",
      "29  .@Azure Maybe raise awareness on this? All App...   \n",
      "\n",
      "                       Category  coded  \n",
      "20                           VM     21  \n",
      "21                           VM     21  \n",
      "22                           VM     21  \n",
      "23                      Support     20  \n",
      "24                       Backup      6  \n",
      "25                           VM     21  \n",
      "26                Other - MS PG     14  \n",
      "27  Visual Studio Team Services     23  \n",
      "28                           VM     21  \n",
      "29                  App Service      2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "\n",
    "values = df['Category'].value_counts()\n",
    "tuples = [tuple((x, y)) for x, y in values.items()]\n",
    "\n",
    "cat_to_remove = []\n",
    "#Making a list of each category that has less than my defined minimum samples\n",
    "for a,b in tuples:\n",
    "    if b < MIN_SAMPLES:\n",
    "        cat_to_remove.append(a)\n",
    "    \n",
    "print(\"Categories to Remove: {}\".format(cat_to_remove))\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "\n",
    "df['Category'] = pd.Categorical(df['Category'])\n",
    "df['coded'] = df['Category'].cat.codes\n",
    "#label = np_utils.to_categorical(data['coded'].as_matrix())\n",
    "\n",
    "\n",
    "coded = dict(enumerate(df['Category'].cat.categories))\n",
    "\n",
    "#I need a list of categories to make TreeMap in d3.js\n",
    "unique_categories = [x for x in coded.values()]\n",
    "num_of_cat = len(unique_categories)\n",
    "print(\"Number of Categories: {}\".format(num_of_cat))\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "print(df[20:30])\n",
    "\n",
    "label = np_utils.to_categorical(df['coded'].as_matrix())\n",
    "\n",
    "cats = le.fit_transform(df['coded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!\n",
      "[1, 59, 70, 317, 3, 439, 844, 9, 5, 36, 354, 60, 4, 42, 152, 5, 650, 650, 8, 1013, 339, 50, 4, 127, 6, 2006, 29, 1159, 12, 1900, 166, 54, 1450]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat = df['TweetText'].tolist()\n",
    "data_1 = df['TweetText'].tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(flat)\n",
    "seq = tokenizer.texts_to_sequences(data_1)\n",
    "print(data_1[12])\n",
    "print(seq[12])\n",
    "\n",
    "#Initialize Word2Vec as a KeyedVector as I won't be using it as an object\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, label, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 10752\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIMENSIONS))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 50, 300)       5781900     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 300, 50, 1)    0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 300, 50, 512)  307712      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 300, 50, 512)  614912      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 300, 50, 512)  768512      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 100, 512)  0           max_pooling2d_1[0][0]            \n",
      "                                                                   max_pooling2d_2[0][0]            \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "                                                                   max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 30, 100, 512)  262656      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalMa (None, 512)           0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           131328      global_max_pooling2d_1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 25)            6425        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 25)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 7,873,445\n",
      "Trainable params: 2,091,545\n",
      "Non-trainable params: 5,781,900\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Embedding, Reshape, Activation, MaxPooling2D, average, Dropout\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import keras\n",
    "\n",
    "#Define Metric for matching accuracy of top 3\n",
    "inTop3 = lambda x, y: top_k_categorical_accuracy(x, y, k=3)\n",
    "\n",
    "input_tweet = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIMENSIONS,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_tweet)\n",
    "dat = Reshape((EMBEDDING_DIMENSIONS,MAX_SEQUENCE_LENGTH,1))(embedding_layer)\n",
    "\n",
    "tower_1 = Conv2D(512, (300, 2), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_1 = MaxPooling2D(pool_size=(10,2))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(512, (300, 4), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_2 = MaxPooling2D(pool_size=(10,2))(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(512, (300, 5), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_3 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "tower_4 = Conv2D(512, (300,1), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_4 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "#cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "cat = keras.layers.concatenate([pool_1, pool_2, pool_3, pool_4], axis=2)\n",
    "conv = Conv2D(512, (1,1), strides=1, padding='same', activation='relu')(cat)\n",
    "pool = GlobalMaxPooling2D()(conv)\n",
    "d_1 = Dense(256)(pool)\n",
    "drop = Dropout(.2)(d_1)\n",
    "dense = Dense(num_of_cat)(drop)\n",
    "out = Activation('softmax')(dense)\n",
    "model_functional = Model(inputs=input_tweet, outputs=out)\n",
    "model_functional.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy', inTop3, 'sparse_categorical_accuracy'])\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10825 samples, validate on 2707 samples\n",
      "Epoch 1/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 2.1589 - acc: 0.3985 - categorical_accuracy: 0.3985 - <lambda>: 0.6128 - sparse_categorical_accuracy: 0.0051Epoch 00000: val_loss improved from inf to 2.23798, saving model to saved_models/weights.best.tweet_classification_clean.hdf5\n",
      "10825/10825 [==============================] - 346s - loss: 2.1587 - acc: 0.3986 - categorical_accuracy: 0.3986 - <lambda>: 0.6127 - sparse_categorical_accuracy: 0.0051 - val_loss: 2.2380 - val_acc: 0.3790 - val_categorical_accuracy: 0.3790 - val_<lambda>: 0.5984 - val_sparse_categorical_accuracy: 0.0033\n",
      "Epoch 2/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 1.8121 - acc: 0.4888 - categorical_accuracy: 0.4888 - <lambda>: 0.6971 - sparse_categorical_accuracy: 0.0199Epoch 00001: val_loss did not improve\n",
      "10825/10825 [==============================] - 345s - loss: 1.8119 - acc: 0.4889 - categorical_accuracy: 0.4889 - <lambda>: 0.6973 - sparse_categorical_accuracy: 0.0199 - val_loss: 2.3373 - val_acc: 0.3857 - val_categorical_accuracy: 0.3857 - val_<lambda>: 0.5999 - val_sparse_categorical_accuracy: 0.0103\n",
      "Epoch 3/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.6092 - categorical_accuracy: 0.6092 - <lambda>: 0.7987 - sparse_categorical_accuracy: 0.0223Epoch 00002: val_loss did not improve\n",
      "10825/10825 [==============================] - 345s - loss: 1.3430 - acc: 0.6092 - categorical_accuracy: 0.6092 - <lambda>: 0.7988 - sparse_categorical_accuracy: 0.0223 - val_loss: 2.7439 - val_acc: 0.3687 - val_categorical_accuracy: 0.3687 - val_<lambda>: 0.5711 - val_sparse_categorical_accuracy: 0.0126\n",
      "Epoch 4/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.9291 - acc: 0.7246 - categorical_accuracy: 0.7246 - <lambda>: 0.8793 - sparse_categorical_accuracy: 0.0213Epoch 00003: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.9291 - acc: 0.7246 - categorical_accuracy: 0.7246 - <lambda>: 0.8793 - sparse_categorical_accuracy: 0.0213 - val_loss: 3.1106 - val_acc: 0.3746 - val_categorical_accuracy: 0.3746 - val_<lambda>: 0.5803 - val_sparse_categorical_accuracy: 0.0155\n",
      "Epoch 5/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.8013 - categorical_accuracy: 0.8013 - <lambda>: 0.9239 - sparse_categorical_accuracy: 0.0213Epoch 00004: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.6644 - acc: 0.8012 - categorical_accuracy: 0.8012 - <lambda>: 0.9238 - sparse_categorical_accuracy: 0.0212 - val_loss: 3.2657 - val_acc: 0.3443 - val_categorical_accuracy: 0.3443 - val_<lambda>: 0.5471 - val_sparse_categorical_accuracy: 0.0103\n",
      "Epoch 6/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.8472 - categorical_accuracy: 0.8472 - <lambda>: 0.9455 - sparse_categorical_accuracy: 0.0201Epoch 00005: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.5204 - acc: 0.8473 - categorical_accuracy: 0.8473 - <lambda>: 0.9455 - sparse_categorical_accuracy: 0.0201 - val_loss: 3.9105 - val_acc: 0.3314 - val_categorical_accuracy: 0.3314 - val_<lambda>: 0.4909 - val_sparse_categorical_accuracy: 0.0081\n",
      "Epoch 7/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.4486 - acc: 0.8697 - categorical_accuracy: 0.8697 - <lambda>: 0.9546 - sparse_categorical_accuracy: 0.0201Epoch 00006: val_loss did not improve\n",
      "10825/10825 [==============================] - 344s - loss: 0.4486 - acc: 0.8697 - categorical_accuracy: 0.8697 - <lambda>: 0.9546 - sparse_categorical_accuracy: 0.0201 - val_loss: 4.2547 - val_acc: 0.3746 - val_categorical_accuracy: 0.3746 - val_<lambda>: 0.5600 - val_sparse_categorical_accuracy: 0.0122\n",
      "Epoch 8/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8816 - categorical_accuracy: 0.8816 - <lambda>: 0.9595 - sparse_categorical_accuracy: 0.0204Epoch 00007: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.4123 - acc: 0.8817 - categorical_accuracy: 0.8817 - <lambda>: 0.9595 - sparse_categorical_accuracy: 0.0204 - val_loss: 3.9901 - val_acc: 0.3532 - val_categorical_accuracy: 0.3532 - val_<lambda>: 0.5242 - val_sparse_categorical_accuracy: 0.0218\n",
      "Epoch 9/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8883 - categorical_accuracy: 0.8883 - <lambda>: 0.9601 - sparse_categorical_accuracy: 0.0201Epoch 00008: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.3950 - acc: 0.8881 - categorical_accuracy: 0.8881 - <lambda>: 0.9600 - sparse_categorical_accuracy: 0.0201 - val_loss: 5.0642 - val_acc: 0.2955 - val_categorical_accuracy: 0.2955 - val_<lambda>: 0.4950 - val_sparse_categorical_accuracy: 0.0100\n",
      "Epoch 10/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8937 - categorical_accuracy: 0.8937 - <lambda>: 0.9618 - sparse_categorical_accuracy: 0.0198Epoch 00009: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.3790 - acc: 0.8937 - categorical_accuracy: 0.8937 - <lambda>: 0.9618 - sparse_categorical_accuracy: 0.0198 - val_loss: 6.2394 - val_acc: 0.3354 - val_categorical_accuracy: 0.3354 - val_<lambda>: 0.5707 - val_sparse_categorical_accuracy: 0.0192\n",
      "Epoch 11/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8958 - categorical_accuracy: 0.8958 - <lambda>: 0.9634 - sparse_categorical_accuracy: 0.0198Epoch 00010: val_loss did not improve\n",
      "10825/10825 [==============================] - 344s - loss: 0.3684 - acc: 0.8958 - categorical_accuracy: 0.8958 - <lambda>: 0.9633 - sparse_categorical_accuracy: 0.0198 - val_loss: 5.5902 - val_acc: 0.2907 - val_categorical_accuracy: 0.2907 - val_<lambda>: 0.4891 - val_sparse_categorical_accuracy: 0.0089\n",
      "Epoch 12/20\n",
      "10820/10825 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.9005 - categorical_accuracy: 0.9005 - <lambda>: 0.9639 - sparse_categorical_accuracy: 0.0197Epoch 00011: val_loss did not improve\n",
      "10825/10825 [==============================] - 343s - loss: 0.3528 - acc: 0.9004 - categorical_accuracy: 0.9004 - <lambda>: 0.9639 - sparse_categorical_accuracy: 0.0197 - val_loss: 5.0960 - val_acc: 0.3569 - val_categorical_accuracy: 0.3569 - val_<lambda>: 0.5294 - val_sparse_categorical_accuracy: 0.0329\n",
      "Epoch 13/20\n",
      " 5400/10825 [=============>................] - ETA: 154s - loss: 0.3619 - acc: 0.9028 - categorical_accuracy: 0.9028 - <lambda>: 0.9631 - sparse_categorical_accuracy: 0.0194"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-764473eff48b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtbCall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_grads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_functional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtbCall\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tweet_classification_clean.hdf5', \n",
    "                    verbose=1, save_best_only=True)\n",
    "\n",
    "tbCall = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=True, embeddings_freq=10, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "model_functional.fit(X_train, y_train, validation_split=.2, batch_size=20, epochs=20, callbacks=[checkpointer, tbCall], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model_functional.save('saved_models/classification_model.h5')\n",
    "\n",
    "def make_cat(pred):\n",
    "    #Takes output prediction Matrix, finds 3 highest values and turns them into the matching category from the previous one-hot encoding\n",
    "    ind = np.argpartition(pred[0], -3)[-3:]\n",
    "    #arr = pred[0][ind]\n",
    "    labels = [coded[int(x)] for x in ind[::-1]]\n",
    "    return labels\n",
    "\n",
    "tweet = ['@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!']\n",
    "pred = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categ = model_functional.predict(np.array(pred))\n",
    "print(\"The Top 3 Categories are {0}\".format(make_cat(categ)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#trained_model = load_model('saved_models/classification_model.h5')\n",
    "#trained_model.summary()\n",
    "#trained_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
