{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "The first step is to take the data and put it into a pandas dataframe. Once the data is into Jupyter, we can begin the process of cleaning and transforming the data into the format our CNN will require. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetText</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@azuresupport #azhelp:</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@AzureSupport, trying to activate a subscripti...</td>\n",
       "      <td>Subscription</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@justinchronicle Hi Justin, our friends @Azure...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@azuresupport #azTechHelp</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@azuresupport #AHD:PT41-JL8</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...</td>\n",
       "      <td>Other - MS PG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@AzureSupport just a heads up, but this button...</td>\n",
       "      <td>Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@AzureSupport I am getting Write DomainService...</td>\n",
       "      <td>AAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TweetText       Category\n",
       "50                          @azuresupport #azTechHelp  Other - MS PG\n",
       "51                             @azuresupport #azhelp:  Other - MS PG\n",
       "52  @AzureSupport, trying to activate a subscripti...   Subscription\n",
       "53  @justinchronicle Hi Justin, our friends @Azure...  Other - MS PG\n",
       "54                          @azuresupport #azTechHelp  Other - MS PG\n",
       "55                          @azuresupport #azTechHelp  Other - MS PG\n",
       "56                        @azuresupport #AHD:PT41-JL8  Other - MS PG\n",
       "57  ã²ã¨ç›®ã§åˆ†ã‹ã‚‹Azure Active Directory ç¬...  Other - MS PG\n",
       "58  @AzureSupport just a heads up, but this button...        Support\n",
       "59  @AzureSupport I am getting Write DomainService...            AAD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors #For word2vec static vectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#CONSTANTS\n",
    "BASE_DIR = '../../../Dataset/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIMENSIONS = 300\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "WEIGHTS_PATH = 'saved_models/weights.best.tweet_classification.hdf5'\n",
    "NUM_OF_RANKS = 25\n",
    "MIN_SAMPLES = 100 #Drop any category with less than 50 samples\n",
    "\n",
    "#Input Data, replace empty with NaN, and then drop these NaN fields so we don't train off non-categorized data.\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "df.dropna(axis=0, how=\"any\", subset=['Category'])\n",
    "df = df.drop(columns=['Date', 'Twitter Handle', 'Link', 'Reach', 'Customer Experience', 'Sentiment', 'Type', 'Total Outbound Tweets'])\n",
    "df[50:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories to Remove: ['Bot Framework', 'CDN', 'Azure Resources Manager', 'IOT Hub', 'VPN Gateway', 'HDInsight', 'SR Complaint', 'Container Service', 'MySQL', 'Site Recovery', 'Notification Hubs', 'Logic Apps', 'Marketplace', 'Media Services', 'Machine Learning', 'Visual Studio Application', 'Data Factory', 'Redis Cache', 'Service Fabric', 'Power BI Embedded', 'PowerShell', 'DreamSpark', 'Free Trial', 'Outage', 'Application Gateway', 'Automation', 'CosmosDB', 'Clear DB', 'Azure Stack', 'Marketing', 'Key Vault', 'Mobile Apps', 'Mobile Services', 'Azure Security Center', 'Powershell', 'Log Analytics', 'Multi-Factor Authentication', 'Imagine', 'Remote App', 'Azure Search', 'Azure DB', 'Load Balancer', 'WordPress', 'Data Lake', 'Stream Analytics', 'Traffic Manager', 'Event Hubs', 'Sign Up Issue', 'Ibizia', 'ExpressRoute', 'Azure Machine Learning', 'Azure Analysis', 'Monitoring', 'SQL Data Warehouse', 'Azure DevTest Labs', 'Compliance', 'Scheduler', 'Operational Insights', 'MSDN', 'Batch', 'Access Control Service', 'AutoScale', 'SQL Stretch Server', 'Data Catalog', 'StorSimple', 'ZUMO App', 'HockeyApp', 'Virtual Machine Scale Sets', 'Biztalk Services', 'Mobile Engagement', 'Minecraft', 'Manage Cache', 'Alerts']\n",
      "Number of Categories: 25\n",
      "                                            TweetText  \\\n",
      "20  @azuresupport #azTechHelp we have no connectio...   \n",
      "21  @AzureSupport Hi! Migrated a Kali Linux VM fro...   \n",
      "22  @azuresupport #azhelp:\\nWhats happening with c...   \n",
      "23  @AzureSupport @andrewwatt BTW, its been logged...   \n",
      "24  @azuresupport #azTechHelp The portal is having...   \n",
      "25  @AzureSupport trying to upload my local window...   \n",
      "26                          @azuresupport #azTechHelp   \n",
      "27  @azuresupport #azTechHelp Estou utilizando um ...   \n",
      "28  @TheRegister are you aware of an Azure VM outa...   \n",
      "29  .@Azure Maybe raise awareness on this? All App...   \n",
      "\n",
      "                       Category  coded  \n",
      "20                           VM     21  \n",
      "21                           VM     21  \n",
      "22                           VM     21  \n",
      "23                      Support     20  \n",
      "24                       Backup      6  \n",
      "25                           VM     21  \n",
      "26                Other - MS PG     14  \n",
      "27  Visual Studio Team Services     23  \n",
      "28                           VM     21  \n",
      "29                  App Service      2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "\n",
    "values = df['Category'].value_counts()\n",
    "tuples = [tuple((x, y)) for x, y in values.items()]\n",
    "\n",
    "cat_to_remove = []\n",
    "#Making a list of each category that has less than my defined minimum samples\n",
    "for a,b in tuples:\n",
    "    if b < MIN_SAMPLES:\n",
    "        cat_to_remove.append(a)\n",
    "    \n",
    "print(\"Categories to Remove: {}\".format(cat_to_remove))\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "\n",
    "df['Category'] = pd.Categorical(df['Category'])\n",
    "df['coded'] = df['Category'].cat.codes\n",
    "#label = np_utils.to_categorical(data['coded'].as_matrix())\n",
    "\n",
    "\n",
    "coded = dict(enumerate(df['Category'].cat.categories))\n",
    "\n",
    "#I need a list of categories to make TreeMap in d3.js\n",
    "unique_categories = [x for x in coded.values()]\n",
    "num_of_cat = len(unique_categories)\n",
    "print(\"Number of Categories: {}\".format(num_of_cat))\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df.Category.isin(cat_to_remove)]\n",
    "print(df[20:30])\n",
    "\n",
    "label = np_utils.to_categorical(df['coded'].as_matrix())\n",
    "\n",
    "cats = le.fit_transform(df['coded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!\n",
      "[1, 59, 70, 317, 3, 439, 844, 9, 5, 36, 354, 60, 4, 42, 152, 5, 650, 650, 8, 1013, 339, 50, 4, 127, 6, 2006, 29, 1159, 12, 1900, 166, 54, 1450]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat = df['TweetText'].tolist()\n",
    "data_1 = df['TweetText'].tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(flat)\n",
    "seq = tokenizer.texts_to_sequences(data_1)\n",
    "print(data_1[12])\n",
    "print(seq[12])\n",
    "\n",
    "#Initialize Word2Vec as a KeyedVector as I won't be using it as an object\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train, X_test, y_train, y_test = train_test_split(seq, label, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 10752\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIMENSIONS))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 50, 300)       5781900     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 300, 50, 1)    0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 300, 50, 512)  307712      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 300, 50, 512)  614912      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 300, 50, 512)  768512      reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 30, 25, 512)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 100, 512)  0           max_pooling2d_1[0][0]            \n",
      "                                                                   max_pooling2d_2[0][0]            \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "                                                                   max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 30, 100, 512)  262656      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalMa (None, 512)           0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           131328      global_max_pooling2d_1[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 25)            6425        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 25)            0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 7,873,445\n",
      "Trainable params: 2,091,545\n",
      "Non-trainable params: 5,781,900\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Embedding, Reshape, Activation, MaxPooling2D, average, Dropout\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import keras\n",
    "\n",
    "#Define Metric for matching accuracy of top 3\n",
    "inTop3 = lambda x, y: top_k_categorical_accuracy(x, y, k=3)\n",
    "\n",
    "input_tweet = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIMENSIONS,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_tweet)\n",
    "dat = Reshape((EMBEDDING_DIMENSIONS,MAX_SEQUENCE_LENGTH,1))(embedding_layer)\n",
    "\n",
    "tower_1 = Conv2D(512, (300, 2), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_1 = MaxPooling2D(pool_size=(10,2))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(512, (300, 4), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_2 = MaxPooling2D(pool_size=(10,2))(tower_2)\n",
    "\n",
    "tower_3 = Conv2D(512, (300, 5), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_3 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "tower_4 = Conv2D(512, (300,1), strides=1, padding='same', activation='relu')(dat)\n",
    "pool_4 = MaxPooling2D(pool_size=(10,2))(tower_3)\n",
    "\n",
    "#cat = keras.layers.concatenate([pool_1, pool_2, pool_3], axis=1)\n",
    "cat = keras.layers.concatenate([pool_1, pool_2, pool_3, pool_4], axis=2)\n",
    "conv = Conv2D(512, (1,1), strides=1, padding='same', activation='relu')(cat)\n",
    "pool = GlobalMaxPooling2D()(conv)\n",
    "d_1 = Dense(256)(pool)\n",
    "drop = Dropout(.2)(d_1)\n",
    "dense = Dense(num_of_cat)(drop)\n",
    "out = Activation('softmax')(dense)\n",
    "model_functional = Model(inputs=input_tweet, outputs=out)\n",
    "model_functional.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy', inTop3, 'sparse_categorical_accuracy'])\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'embeddings_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-34a7ca732677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                     verbose=1, save_best_only=True)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtbCall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_grads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel_functional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtbCall\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'embeddings_data'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.tweet_classification_clean.hdf5', \n",
    "                    verbose=1, save_best_only=True)\n",
    "\n",
    "tbCall = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=True, embeddings_freq=10, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "model_functional.fit(X_train, y_train, validation_split=.2, batch_size=20, epochs=20, callbacks=[checkpointer, tbCall], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model_functional.save('saved_models/classification_model.h5')\n",
    "\n",
    "def make_cat(pred):\n",
    "    #Takes output prediction Matrix, finds 3 highest values and turns them into the matching category from the previous one-hot encoding\n",
    "    ind = np.argpartition(pred[0], -3)[-3:]\n",
    "    #arr = pred[0][ind]\n",
    "    labels = [coded[int(x)] for x in ind[::-1]]\n",
    "    return labels\n",
    "\n",
    "tweet = ['@AzureSupport Im getting different Azure Search results in the portal vs what I get via the SDK. SDK is returning nothing when I add a filter. Any tips on debugging would be appreciated!']\n",
    "pred = pad_sequences(tokenizer.texts_to_sequences(tweet), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categ = model_functional.predict(np.array(pred))\n",
    "print(\"The Top 3 Categories are {0}\".format(make_cat(categ)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#trained_model = load_model('saved_models/classification_model.h5')\n",
    "#trained_model.summary()\n",
    "#trained_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
